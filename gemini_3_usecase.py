# -*- coding: utf-8 -*-
"""gemini_3_usecase

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/embedded/projects/demos-vertex/locations/us-central1/repositories/af07727d-b73b-43c1-b1fa-fb5295a7957c
"""

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""### Install Google Gen AI SDK for Python

Gemini 3 API features require Gen AI SDK for Python version 1.51.0 or later.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade --quiet google-genai

"""### Import libraries"""

import os
import sys

from IPython.display import HTML, Markdown, display
from google import genai
from google.genai import types
from pydantic import BaseModel

"""### Authenticate your Google Cloud Project for Vertex AI

You can use a Google Cloud Project or an API Key for authentication. This notebook uses a Google Cloud Project.

- [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)
"""

# fmt: off
PROJECT_ID = ""  # @param {type: "string", placeholder: "[your-project-id]", isTemplate: true}
# fmt: on
if not PROJECT_ID or PROJECT_ID == "[your-project-id]":
    PROJECT_ID = str(os.environ.get("GOOGLE_CLOUD_PROJECT"))

LOCATION = "global"

client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)

"""### Choose a Gemini 3 Pro model

Use `gemini-3-pro-preview` in this tutorial. Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models).
"""

MODEL_ID = "gemini-3-pro-preview"  # @param ["gemini-3-pro-preview"] {type: "string"}

prompt = """
Act as a Strategic Assistant. Analyze this handwritten daily plan

1. Morning Briefing & Status Update: Identify the core objective and current
   status based on visual cues (e.g., icons, emphasis markers).
2. The Priority Stack:: List fixed time blocks, identifying any hard
   deadlines and tentative placeholders.
3. Productivity Guardrails: Identify and interpret  Global
   Execution Rules or time boundaries.
4. Real-time Interventions: Identify unplanned disruptions or incoming queries
    and assess their impact on the schedule.
5. Actionable Triggers: Map symbols (?, !!, ⚠) to specific tasks that
   require external data or represent critical risks.

Output the result as a 'Strategic Daily Briefing' that is concise and
action-oriented. Make sure you have covered all the points
"""

"""### ✅ Note

- If your content is stored in [Google Cloud Storage](https://cloud.google.com/storage), you can use the `from_uri`  method to create a `Part` object.
- If your content is stored in your local file system, you can read it in as bytes data and use the `from_bytes` method to create a `Part` object.


```
# Download and open an image locally.
! wget https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png

with open("meal.png", "rb") as f:
    image = f.read()

response = client.models.generate_content(
    model=MODEL_ID,
    contents=[
        types.Part.from_bytes(data=image, mime_type="image/png"),
        "Write a short and engaging blog post based on this picture.",
    ],
)

display(Markdown(response.text))
```


"""

response = client.models.generate_content(
    model=MODEL_ID,
    config=types.GenerateContentConfig(
        temperature=1.0,
        top_p=0.9,
        max_output_tokens=8000,
        seed=42
    ),
    contents=[
        types.Part(
            file_data=types.FileData(
                file_uri="gs://ai_camp_demo_artefacts/tasklist_1.jpg",
                mime_type="image/jpeg",
            ),
            media_resolution=types.PartMediaResolution(
                level=types.PartMediaResolutionLevel.MEDIA_RESOLUTION_HIGH,
            ),
        ),
        prompt
    ],
)

display(Markdown(response.text))

image_generation = client.models.generate_content(
    model="gemini-3-pro-image-preview",
    contents=f"""
    Generate an infographic of the daily schedule: {response.text}.
    Stick to the facts and do not miss any steps
    """,
    config=types.GenerateContentConfig(
        image_config=types.ImageConfig(
            aspect_ratio="16:9",
            image_size="4K"
        )
    )
)

image_parts = [part for part in image_generation.parts if part.inline_data]

if image_parts:
    image = image_parts[0].as_image()
    image.save('infographic_schedule_1.png')
    image.show()

"""### Take Home Task
What if you use low resolution?
What variation do you see in the response?
"""

# response = client.models.generate_content(
#     model=MODEL_ID,
#     contents=[
#         types.Part(
#             file_data=types.FileData(
#                 file_uri="gs://ai_camp_demo_artefacts/tasklist.jpg",
#                 mime_type="image/jpeg",
#             ),
#             media_resolution=types.PartMediaResolution(
#                 level=types.PartMediaResolutionLevel.MEDIA_RESOLUTION_LOW
#             ),
#         ),
#         prompt
#     ],
# )

# display(Markdown(response.text))

prompt_1 = """
Look at the red-ink interruption and the 'Customer Questions' note. Based on the 1:30 PM slide deadline,
what is the latest possible time I can address these without missing my 2:00 PM review?
"""

reasoning_response = client.models.generate_content(
    model=MODEL_ID,
    contents=[
        types.Part(
            file_data=types.FileData(
                file_uri="gs://ai_camp_demo_artefacts/tasklist_1.jpg",
                mime_type="image/jpeg",
            ),
            media_resolution=types.PartMediaResolution(
                level=types.PartMediaResolutionLevel.MEDIA_RESOLUTION_HIGH
            ),
        ),
        prompt_1
    ],
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(
            include_thoughts = True,
            thinking_level=types.ThinkingLevel.HIGH  # Dynamic thinking for high reasoning tasks
        )
    ),
)

"""Show thoughts and response"""

for part in reasoning_response.candidates[0].content.parts:
    if part.thought:
        display(
            Markdown(
                f"""## Thoughts:
         {part.text}
        """
            )
        )
    else:
        display(
            Markdown(
                f"""## Answer:
         {part.text}
        """
            )
        )

image_generation = client.models.generate_content(
    model="gemini-3-pro-image-preview",
    contents=f"Generate an infographic of the daily schedule: {reasoning_response.text}.",
    config=types.GenerateContentConfig(
        image_config=types.ImageConfig(
            aspect_ratio="16:9",
            image_size="4K"
        )
    )
)

image_parts = [part for part in image_generation.parts if part.inline_data]

if image_parts:
    image = image_parts[0].as_image()
    image.save('infographic_reason_1.png')
    image.show()